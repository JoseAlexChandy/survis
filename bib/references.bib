@article{0f721dae0f09b428c9b8aed1380ace52bbb76b30,
title = {Model-free and learning-free grasping by Local Contact Moment matching},
year = {2018},
url = {https://www.semanticscholar.org/paper/0f721dae0f09b428c9b8aed1380ace52bbb76b30},
abstract = {This paper addresses the problem of grasping arbitrarily shaped objects, observed as partial point-clouds, without requiring: models of the objects, physics parameters, training data, or other a-priori knowledge. A grasp metric is proposed based on Local Contact Moment (LoCoMo). LoCoMo combines zero-moment shift features, of both hand and object surface patches, to determine local similarity. This metric is then used to search for a set of feasible grasp poses with associated grasp likelihoods. LoCoMo overcomes some limitations of both classical grasp planners and learning-based approaches. Unlike force-closure analysis, LoCoMo does not require knowledge of physical parameters such as friction coefficients, and avoids assumptions about fingertip contacts, instead enabling robust contacts of large areas of hand and object surface. Unlike more recent learning-based approaches, LoCoMo does not require training data, and does not need any prototype grasp configurations to be taught by kinesthetic demonstration. We present results of real-robot experiments grasping 21 different objects, observed by a wrist-mounted depth camera. All objects are grasped successfully when presented to the robot individually. The robot also successfully clears cluttered heaps of objects by sequentially grasping and lifting objects until none remain.},
author = {Maxime Adjigble and N. Marturi and V. Ortenzi and Vijaykumar Rajasekaran and Peter Corke and R. Stolkin},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
keywords = {visual design: bar chart},
volume = {null},
pages = {2933-2940},
doi = {10.1109/IROS.2018.8594226},
}

@article{c0aae1b0e1d46cf06c2ebfdff2f35a0978fc51a2,
title = {Planning High-Quality Grasps Using Mean Curvature Object Skeletons},
year = {2017},
url = {https://www.semanticscholar.org/paper/c0aae1b0e1d46cf06c2ebfdff2f35a0978fc51a2},
abstract = {In this letter, we present a grasp planner that integrates two sources of information to generate robust grasps for a robotic hand. First, the topological information of the object model is incorporated by building the mean curvature skeleton and segmenting the object accordingly in order to identify object regions which are suitable for grasping. We show how this information can be used to derive different grasping strategies, which also allows to distinguish between precision and power grasps. Second, the local surface structure is investigated to construct feasible and robust grasping poses by aligning the hand according to the local object shape. We apply the approach to a wide variety of object models of the KIT and the YCB real-world object model databases and evaluate it with several robotic hands. The results show that the skeleton-based grasp planner is capable to generate high-quality grasps in an efficient manner. In addition, we evaluate how robust the planned grasps are against hand positioning errors as they occur in real-world applications due to perception and actuation inaccuracies. The evaluation shows that the majority of the generated grasps are of high quality since they can be successfully applied even when the hand is not exactly positioned.},
author = {N. Vahrenkamp and E. Koch and Mirko Wächter and T. Asfour},
journal = {IEEE Robotics and Automation Letters},
volume = {3},
pages = {911-918},
doi = {10.1109/LRA.2018.2792694},
arxivid = {1710.02418},
}

@article{83e53c519171a25cd5a210afc39c90bfa552e1a2,
title = {A Benchmarking Framework for Systematic Evaluation of Robotic Pick-and-Place Systems in an Industrial Grocery Setting},
year = {2019},
url = {https://www.semanticscholar.org/paper/83e53c519171a25cd5a210afc39c90bfa552e1a2},
abstract = {Robotic manipulation is a very active field of research nowadays; however, pick-and-place operations constitute the majority of today’s industrial robotic applications. In order to adopt a robotic solution for an industrial setting, proper evaluation processes should be defined to assess the system’s performance. A number of benchmarks have been proposed in the literature focusing mainly on individual components needed to perform the task, like grasping, perception and motion planning; thus, they do not provide enough information on the performance of the entire robotic system. To address this, we propose a benchmarking framework for a pick-and-place task inspired by a use case for picking fruits and vegetables in an industrial setting. To foster reproducible research and comparison of different robotic systems, the benchmarking framework uses surrogate objects with instructions on how to build them, an easy-to-reproduce environment, and guidelines for object placement. The proposed benchmark is applied to evaluate the performance of two variants of a robotic system with different end-effectors.},
author = {P. Triantafyllou and H. Mnyusiwalla and P. Sotiropoulos and M. Roa and Duncan Russell and G. Deacon},
journal = {2019 International Conference on Robotics and Automation (ICRA)},
volume = {null},
pages = {6692-6698},
doi = {10.1109/ICRA.2019.8793993},
}

@article{1b13cd3dcb46945e7c5c2b2f6642d8f2da1d1185,
title = {Simultaneous Object Reconstruction and Grasp Prediction using a Camera-centric Object Shell Representation},
year = {2021},
url = {https://www.semanticscholar.org/paper/1b13cd3dcb46945e7c5c2b2f6642d8f2da1d1185},
abstract = {Being able to grasp objects is a fundamental component of most robotic manipulation systems. In this paper, we present a new approach to simultaneously reconstruct a mesh and a dense grasp quality map of an object from a depth image. At the core of our approach is a novel camera-centric object representation called the “object shell” which is composed of an observed “entry image” and a predicted “exit image”. We present an image-to-image residual ConvNet architecture in which the object shell and a grasp-quality map are predicted as separate output channels. The main advantage of the shell representation and the corresponding neural network architecture, ShellGrasp-Net, is that the input-output pixel correspondences in the shell representation are explicitly represented in the architecture. We show that this coupling yields superior generalization capabilities for object reconstruction and accurate grasp quality estimation implicitly considering the object geometry. Our approach yields an efficient dense grasp quality map and an object geometry estimate in a single forward pass. Both of these outputs can be used in a wide range of robotic manipulation applications. With rigorous experimental validation, both in simulation and on a real setup, we show that our shell-based method can be used to generate precise grasps and the associated grasp quality with over 90% accuracy. Diverse grasps computed on shell reconstructions allow the robot to select and execute grasps in cluttered scenes with more than 93% success rate.},
author = {Nikhil Chavan-Dafle and S. Popovych and Shubham Agrawal and Daniel D. Lee and Volkan Isler},
journal = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
volume = {null},
pages = {1396-1403},
doi = {10.1109/IROS47612.2022.9981955},
arxivid = {2109.06837},
}

@article{95a721ad687df97eda87d432b42b0b787a44a5de,
title = {DDGC: Generative Deep Dexterous Grasping in Clutter},
year = {2021},
url = {https://www.semanticscholar.org/paper/95a721ad687df97eda87d432b42b0b787a44a5de},
abstract = {Recent advances in multi-fingered robotic grasping have enabled fast 6-Degrees-of-Freedom (DOF) single object grasping. Multi-finger grasping in cluttered scenes, on the other hand, remains mostly unexplored due to the added difficulty of reasoning over obstacles which greatly increases the computational time to generate high-quality collision-free grasps. In this work, we address such limitations by introducing DDGC, a fast generative multi-finger grasp sampling method that can generate high quality grasps in cluttered scenes from a single RGB-D image. DDGC is built as a network that encodes scene information to produce coarse-to-fine collision-free grasp poses and configurations. We experimentally benchmark DDGC against two state-of-the-art methods on 1200 simulated cluttered scenes and 7 real-world scenes. The results show that DDGC outperforms the baselines in synthesizing high-quality grasps and removing clutter. DDGC is also 4-5 times faster than GraspIt!. This, in turn, opens the door for using multi-finger grasps in practical applications which has so far been limited due to the excessive computation time needed by other methods. Code and videos are available at https://irobotics.aalto.fi/ddgc/.},
author = {Jens Lundell and Francesco Verdoja and V. Kyrki},
journal = {IEEE Robotics and Automation Letters},
volume = {6},
pages = {6899-6906},
doi = {10.1109/LRA.2021.3096239},
arxivid = {2103.04783},
}

@article{73c3a9e084a8afa9831f07cd748eec3e4e2ef448,
title = {A Bin-Picking Benchmark for Systematic Evaluation of Robotic Pick-and-Place Systems},
year = {2020},
url = {https://www.semanticscholar.org/paper/73c3a9e084a8afa9831f07cd748eec3e4e2ef448},
abstract = {Pick-and-place operations constitute the majority of today's industrial robotic applications. However, comparability and reproducibility of results has remained an issue that delays further advances in this field. Evaluation of manipulation systems can be carried out at different levels, but for the final application the performance of the overall system is the critical one. This paper proposes a benchmarking framework for pick-and-place systems, inspired by a typical task in the logistic domain: picking up fruits and vegetables from a container and placing them in an order bin. The framework uses an easy-to-reproduce environment, a publicly available object set, and guidelines for creating scenarios of different complexity. The proposed benchmark is applied to evaluate the performance of four variants of a robotic system with different end-effectors.},
author = {H. Mnyusiwalla and P. Triantafyllou and P. Sotiropoulos and M. Roa and W. Friedl and A. M. Sundaram and Duncan Russell and G. Deacon},
journal = {IEEE Robotics and Automation Letters},
volume = {5},
pages = {1389-1396},
doi = {10.1109/LRA.2020.2965076},
}

@article{5641a19a6df3b68ff42c07625eeefdafef61643e,
title = {Benchmark for Bimanual Robotic Manipulation of Semi-Deformable Objects},
year = {2020},
url = {https://www.semanticscholar.org/paper/5641a19a6df3b68ff42c07625eeefdafef61643e},
abstract = {We propose a new benchmarking protocol to evaluate algorithms for bimanual robotic manipulation semi-deformable objects. The benchmark is inspired from two real-world applications: (a) watchmaking craftsmanship, and (b) belt assembly in automobile engines. We provide two setups that try to highlight the following challenges: (a) manipulating objects via a tool, (b) placing irregularly shaped objects in the correct groove, (c) handling semi-deformable objects, and (d) bimanual coordination. We provide CAD drawings of the task pieces that can be easily 3D printed to ensure ease of reproduction, and detailed description of tasks and protocol for successful reproduction, as well as meaningful metrics for comparison. We propose four categories of submission in an attempt to make the benchmark accessible to a wide range of related fields spanning from adaptive control, motion planning to learning the tasks through trial-and-error learning.},
author = {Konstantinos Chatzilygeroudis and Bernardo Fichera and Ilaria Lauzana and Fanjun Bu and Kunpeng Yao and Farshad Khadivar and A. Billard},
journal = {IEEE Robotics and Automation Letters},
volume = {5},
pages = {2443-2450},
doi = {10.1109/LRA.2020.2972837},
}

@article{456cfcbc4c5f6513b36d744456387b95d126f973,
title = {Household Cloth Object Set: Fostering Benchmarking in Deformable Object Manipulation},
year = {2021},
url = {https://www.semanticscholar.org/paper/456cfcbc4c5f6513b36d744456387b95d126f973},
abstract = {Benchmarking of robotic manipulations is one of the open issues in robotic research. An important factor that has enabled progress in this area in the last decade is the existence of common object sets that have been shared among different research groups. However, the existing object sets are very limited when it comes to cloth-like objects that have unique particularities and challenges. This letter is a first step towards the design of a cloth object set to be distributed among research groups from the robotics cloth manipulation community. We present a set of household cloth objects and related tasks that serve to expose the challenges related to gathering such an object set and propose a roadmap to the design of common benchmarks in cloth manipulation tasks, with the intention to set the grounds for a future debate in the community that will be necessary to foster benchmarking for the manipulation of cloth-like objects. Some RGB-D and object scans are collected as examples for the relevant configurations and shared in http://www.iri.upc.edu/groups/perception/ClothObjectSet/.},
author = {Irene Garcia-Camacho and J. Borràs and B. Çalli and Adam Norton and G. Alenyà},
journal = {IEEE Robotics and Automation Letters},
volume = {7},
pages = {5866-5873},
doi = {10.1109/LRA.2022.3158428},
arxivid = {2111.01527},
}

@article{7df6053722fc35de05eea769fd29aece0045073b,
title = {Benchmarking Protocol for Grasp Planning Algorithms},
year = {2020},
url = {https://www.semanticscholar.org/paper/7df6053722fc35de05eea769fd29aece0045073b},
abstract = {Numerous grasp planning algorithms have been proposed since the 1980s. The grasping literature has expanded rapidly in recent years, building on greatly improved vision systems and computing power. Methods have been proposed to plan stable grasps on known objects (exact 3D model is available), familiar objects (e.g. exploiting a-priori known grasps for different objects of the same category), or novel object shapes observed during task execution. Few of these methods have ever been compared in a systematic way, and objective performance evaluation of such complex systems remains problematic. Difficulties and confounding factors include different assumptions and amounts of a-priori knowledge in different algorithms; different robots, hands, vision systems and setups in different labs; and different choices or application needs for grasped objects. Also, grasp planning can use different grasp quality metrics (including empirical or theoretical stability measures) or other criteria, e.g., computational speed, or combination of grasps with reachability considerations. While acknowledging and discussing the outstanding difficulties surrounding this complex topic, we propose a methodology for reproducible experiments to compare the performance of a variety of grasp planning algorithms. Our protocol attempts to improve the objectivity with which different grasp planners are compared by minimizing the influence of key components in the grasping pipeline, e.g., vision and pose estimation. The protocol is demonstrated by evaluating two different grasp planners: a state-of-the-art model-free planner and a popular open-source model-based planner. We show results from real-robot experiments with a 7-DoF arm and 2-finger hand, as well as simulation-based evaluations.},
author = {Yasemin Bekiroglu and N. Marturi and M. Roa and Komlan Jean Maxime Adjigble and T. Pardi and C. Grimm and R. Balasubramanian and Kaiyu Hang and R. Stolkin},
journal = {IEEE Robotics and Automation Letters},
volume = {5},
pages = {315-322},
doi = {10.1109/LRA.2019.2956411},
}

@article{539f627fd1f56588ea7ad41e676ed37bba461eed,
title = {GRASPA 1.0: GRASPA is a Robot Arm graSping Performance BenchmArk},
year = {2020},
url = {https://www.semanticscholar.org/paper/539f627fd1f56588ea7ad41e676ed37bba461eed},
abstract = {The use of benchmarks is a widespread and scientifically meaningful practice to validate performance of different approaches to the same task. In the context of robot grasping the use of common object sets has emerged in recent years, however no dominant protocols and metrics to test grasping pipelines have taken root yet. In this letter, we present version 1.0 of GRASPA, a benchmark to test effectiveness of grasping pipelines on physical robot setups. This approach tackles the complexity of such pipelines by proposing different metrics that account for the features and limits of the test platform. As an example application, we deploy GRASPA on the iCub humanoid robot and use it to benchmark our grasping pipeline. As closing remarks, we discuss how the GRASPA indicators we obtained as outcome can provide insight into how different steps of the pipeline affect the overall grasping performance.},
author = {Fabrizio Bottarel and G. Vezzani and U. Pattacini and L. Natale},
journal = {IEEE Robotics and Automation Letters},
volume = {5},
pages = {836-843},
doi = {10.1109/LRA.2020.2965865},
arxivid = {2002.05017},
}
